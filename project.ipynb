{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os # for creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0') # initialise environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 32\n",
    "n_episodes = 1001 # n games we want agent to play (default 1001)\n",
    "output_dir = 'model_output/mountaincar/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000) \n",
    "        self.epsilon = 1.0\n",
    "        self.model = self._build_model() \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu')) \n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear')) \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            act_values = self.model.predict(state) \n",
    "        \n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size): \n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) \n",
    "#         minibatch = self.memory \n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch: \n",
    "        \n",
    "            if done:\n",
    "                target = reward\n",
    "            else: \n",
    "                target = reward + 0.97 * np.amax(self.model.predict(next_state)[0])\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "        \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "#         if self.epsilon > 0.01:\n",
    "#             self.epsilon = self.epsilon * 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1001, score: -0.58, e: 1.0\n",
      "episode: 1/1001, score: -0.62, e: 1.0\n",
      "episode: 2/1001, score: -0.53, e: 1.0\n",
      "episode: 3/1001, score: -0.46, e: 1.0\n",
      "episode: 4/1001, score: -0.39, e: 1.0\n",
      "episode: 5/1001, score: -0.52, e: 1.0\n",
      "episode: 6/1001, score: -0.64, e: 1.0\n",
      "episode: 7/1001, score: -0.55, e: 1.0\n",
      "episode: 8/1001, score: -0.68, e: 1.0\n",
      "episode: 9/1001, score: -0.51, e: 1.0\n",
      "episode: 10/1001, score: -0.53, e: 1.0\n",
      "episode: 11/1001, score: -0.59, e: 1.0\n",
      "episode: 12/1001, score: -0.57, e: 1.0\n",
      "episode: 13/1001, score: -0.4, e: 1.0\n",
      "episode: 14/1001, score: -0.3, e: 1.0\n",
      "episode: 15/1001, score: -0.54, e: 1.0\n",
      "episode: 16/1001, score: -0.51, e: 1.0\n",
      "episode: 17/1001, score: -0.57, e: 1.0\n",
      "episode: 18/1001, score: -0.56, e: 1.0\n",
      "episode: 19/1001, score: -0.43, e: 1.0\n",
      "episode: 20/1001, score: -0.47, e: 1.0\n",
      "episode: 21/1001, score: -0.45, e: 1.0\n",
      "episode: 22/1001, score: -0.63, e: 1.0\n",
      "episode: 23/1001, score: -0.51, e: 1.0\n",
      "episode: 24/1001, score: -0.63, e: 1.0\n",
      "episode: 25/1001, score: -0.59, e: 1.0\n",
      "episode: 26/1001, score: -0.63, e: 1.0\n",
      "episode: 27/1001, score: -0.49, e: 1.0\n",
      "episode: 28/1001, score: -0.54, e: 1.0\n",
      "episode: 29/1001, score: -0.73, e: 1.0\n",
      "episode: 30/1001, score: -0.68, e: 1.0\n",
      "episode: 31/1001, score: -0.42, e: 1.0\n",
      "episode: 32/1001, score: -0.51, e: 1.0\n",
      "episode: 33/1001, score: -0.66, e: 1.0\n",
      "episode: 34/1001, score: -0.52, e: 1.0\n",
      "episode: 35/1001, score: -0.58, e: 1.0\n",
      "episode: 36/1001, score: -0.54, e: 1.0\n",
      "episode: 37/1001, score: -0.46, e: 1.0\n",
      "episode: 38/1001, score: -0.44, e: 1.0\n",
      "episode: 39/1001, score: -0.6, e: 1.0\n",
      "episode: 40/1001, score: -0.56, e: 1.0\n",
      "episode: 41/1001, score: -0.7, e: 1.0\n",
      "episode: 42/1001, score: -0.62, e: 1.0\n",
      "episode: 43/1001, score: -0.41, e: 1.0\n",
      "episode: 44/1001, score: -0.66, e: 1.0\n",
      "episode: 45/1001, score: -0.62, e: 1.0\n",
      "episode: 46/1001, score: -0.49, e: 1.0\n",
      "episode: 47/1001, score: -0.41, e: 1.0\n",
      "episode: 48/1001, score: -0.42, e: 1.0\n",
      "episode: 49/1001, score: -0.63, e: 1.0\n",
      "episode: 50/1001, score: -0.67, e: 1.0\n",
      "episode: 51/1001, score: -0.42, e: 1.0\n",
      "episode: 52/1001, score: -0.52, e: 1.0\n",
      "episode: 53/1001, score: -0.56, e: 1.0\n",
      "episode: 54/1001, score: -0.51, e: 1.0\n",
      "episode: 55/1001, score: -0.49, e: 1.0\n",
      "episode: 56/1001, score: -0.54, e: 1.0\n",
      "episode: 57/1001, score: -0.54, e: 1.0\n",
      "episode: 58/1001, score: -0.64, e: 1.0\n",
      "episode: 59/1001, score: -0.61, e: 1.0\n",
      "episode: 60/1001, score: -0.63, e: 1.0\n",
      "episode: 61/1001, score: -0.53, e: 1.0\n",
      "episode: 62/1001, score: -0.6, e: 1.0\n",
      "episode: 63/1001, score: -0.48, e: 1.0\n",
      "episode: 64/1001, score: -0.75, e: 1.0\n",
      "episode: 65/1001, score: -0.59, e: 1.0\n",
      "episode: 66/1001, score: -0.5, e: 1.0\n",
      "episode: 67/1001, score: -0.6, e: 1.0\n",
      "episode: 68/1001, score: -0.51, e: 1.0\n",
      "episode: 69/1001, score: -0.58, e: 1.0\n",
      "episode: 70/1001, score: -0.44, e: 1.0\n",
      "episode: 71/1001, score: -0.49, e: 1.0\n",
      "episode: 72/1001, score: -0.3, e: 1.0\n",
      "episode: 73/1001, score: -0.39, e: 1.0\n",
      "episode: 74/1001, score: -0.6, e: 1.0\n",
      "episode: 75/1001, score: -0.63, e: 1.0\n",
      "episode: 76/1001, score: -0.59, e: 1.0\n",
      "episode: 77/1001, score: -0.44, e: 1.0\n",
      "episode: 78/1001, score: -0.68, e: 1.0\n",
      "episode: 79/1001, score: -0.63, e: 1.0\n",
      "episode: 80/1001, score: -0.43, e: 1.0\n",
      "episode: 81/1001, score: -0.35, e: 1.0\n",
      "episode: 82/1001, score: -0.8, e: 1.0\n",
      "episode: 83/1001, score: -0.58, e: 1.0\n",
      "episode: 84/1001, score: -0.42, e: 1.0\n",
      "episode: 85/1001, score: -0.4, e: 1.0\n",
      "episode: 86/1001, score: -0.66, e: 1.0\n",
      "episode: 87/1001, score: -0.54, e: 1.0\n",
      "episode: 88/1001, score: -0.57, e: 1.0\n",
      "episode: 89/1001, score: -0.68, e: 1.0\n",
      "episode: 90/1001, score: -0.37, e: 1.0\n",
      "episode: 91/1001, score: -0.44, e: 1.0\n",
      "episode: 92/1001, score: -0.48, e: 1.0\n",
      "episode: 93/1001, score: -0.49, e: 1.0\n",
      "episode: 94/1001, score: -0.4, e: 1.0\n",
      "episode: 95/1001, score: -0.59, e: 1.0\n",
      "episode: 96/1001, score: -0.63, e: 1.0\n",
      "episode: 97/1001, score: -0.77, e: 1.0\n",
      "episode: 98/1001, score: -0.55, e: 1.0\n",
      "episode: 99/1001, score: -0.57, e: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-25aa00dea52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     95\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3334\u001b[0m     \"\"\"\n\u001b[1;32m   3335\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3336\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m     \"\"\"\n\u001b[0;32m-> 3225\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \"\"\"\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size) # initialise agent\n",
    "## done = False\n",
    "successes = 0\n",
    "for e in range(n_episodes): \n",
    "    state = env.reset()\n",
    "    s1 = state\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for time in range(500): \n",
    "#         env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        ns1 = next_state\n",
    "        reward = next_state[0] + 0.5\n",
    "        if next_state[0] >= 0.5:\n",
    "            reward += 1\n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "       \n",
    "        state = next_state   \n",
    "        \n",
    "        target = reward\n",
    "        target_f = agent.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        agent.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if done:\n",
    "            if  ns1[0] >= 0.5:\n",
    "                if agent.epsilon > 0.01:\n",
    "                    agent.epsilon *= .995\n",
    "#             successes += 1\n",
    "\n",
    "           \n",
    "            print(\"episode: {}/{}, score: {:.2}, e: {:.2}\".format(e, n_episodes, ns1[0], agent.epsilon))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
